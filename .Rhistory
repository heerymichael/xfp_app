ypt = NA_real_
) %>%
select(
player, position, team, week, season, receiver_id,
xFP, actual_FP,
target, air_yards, rz_target, receptions,
receiving_yards, receiving_touchdowns,
carries, rush_yards, rush_touchdowns, goal_line_carry,
adot, catch_rate, ypt
)
# Combine and standardize
combined_xfp_data <- bind_rows(wr_te_qb_data, rb_combined_data) %>%
mutate(
# Standardize team names
team = case_when(
team == "LA" ~ "LAR",
team == "STL" ~ "LAR",
team == "SD" ~ "LAC",
team == "OAK" ~ "LV",
TRUE ~ team
),
# Add calculated fields
xFP = as.numeric(xFP),
actual_FP = as.numeric(actual_FP),
fp_diff = actual_FP - xFP,
games = 1
) %>%
arrange(team, week, desc(xFP))
cat("✓ Combined xFP data processed.\n")
cat("  - Total rows:", nrow(combined_xfp_data), "\n")
cat("  - Unique players:", n_distinct(combined_xfp_data$player), "\n")
cat("  - Weeks included:", paste(sort(unique(combined_xfp_data$week)), collapse = ", "), "\n\n")
# Upload to public Google Sheet ====
cat("Step 3: Uploading processed data to public Google Sheet...\n")
# Clear existing sheet and write new data
sheet_write(
combined_xfp_data,
ss = target_sheet_url,
sheet = "combined_xfp_data"
)
cat("✓ Data successfully uploaded to Google Sheet.\n\n")
# Optional: Save local backup
cat("Step 4: Saving local backup...\n")
dir.create("data", showWarnings = FALSE)
write_csv(combined_xfp_data, "data/combined_xfp_data_backup.csv")
cat("✓ Local backup saved to data/combined_xfp_data_backup.csv\n\n")
# Summary
cat("====================================\n")
cat("UPDATE COMPLETE!\n")
cat("====================================\n")
cat("Processed", nrow(combined_xfp_data), "rows of data\n")
cat("Data is now available at:\n")
cat(target_sheet_url, "\n\n")
cat("Your Shiny app will automatically use this updated data.\n")
cat("No need to redeploy the app!\n")
shiny::runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
# Required libraries
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
# Function to scrape a given FBref shooting table for a league + season
scrape_fbref_shooting <- function(league_url, pause_sec = 2) {
# league_url: full url to the “shooting” stats page for a league & season
# pause_sec: seconds to wait between requests to avoid overloading FBref
message("Fetching: ", league_url)
# read page
page <- read_html(league_url)
# find the shooting table (player level). On FBref, usually table id = "stats_shooting"
# It may vary: check via View Source in the browser.
tbl <- page %>%
html_element("table#stats_shooting") %>%
html_table(fill = TRUE)
# Clean up column names
tbl_clean <- tbl %>%
janitor::clean_names() %>%
# sometimes there’s a header row inside the table; drop rows that are repeated header
filter(player != "Player") %>%
# convert numeric columns
mutate(across(where(is.character), ~ str_trim(.))) %>%
# convert appropriate columns to numeric
mutate(
shots = as.numeric(shots),
shots_on_target = as.numeric(shots_on_target),
xg = as.numeric(xg),
npxg = as.numeric(npxg),
# etc. add other columns you need
# you might need to check column names, because sometimes column names change, e.g. "non-penalty_xg"
)
# optional: pause before returning
Sys.sleep(pause_sec)
return(tbl_clean)
}
url_prem_shooting_25_26 <- "https://fbref.com/en/comps/9/2025-2026/shooting/2025-2026-Premier-League-Stats"
url_serieA_shooting_25_26 <- "https://fbref.com/en/comps/11/2025-2026/shooting/2025-2026-Serie-A-Stats"
prem_shooting <- scrape_fbref_shooting(url_prem_shooting_25_26, pause_sec = 3)
scrape_fbref_shooting <- function(league_url, pause_sec = 2) {
message("Fetching: ", league_url)
# download raw HTML
page <- read_html(league_url)
# FBref tables are inside comments -> extract comments
comments <- page %>%
html_nodes(xpath = "//comment()") %>%
html_text()
# Find the one that contains the shooting table
shooting_html <- comments[str_detect(comments, "table id=\"stats_shooting\"")]
if (length(shooting_html) == 0) {
stop("Could not find shooting table in commented HTML")
}
# Parse the shooting table HTML
shooting_page <- read_html(shooting_html)
tbl <- shooting_page %>%
html_element("table#stats_shooting") %>%
html_table(fill = TRUE)
# Clean up
tbl_clean <- tbl %>%
clean_names() %>%
filter(player != "Player") %>%
mutate(across(where(is.character), str_trim)) %>%
mutate(
shots = suppressWarnings(as.numeric(shots)),
shots_on_target = suppressWarnings(as.numeric(shots_on_target)),
xg = suppressWarnings(as.numeric(xg)),
npxg = suppressWarnings(as.numeric(npxg))
)
Sys.sleep(pause_sec)
return(tbl_clean)
}
# Example use
url_prem_shooting_25_26 <- "https://fbref.com/en/comps/9/2025-2026/shooting/2025-2026-Premier-League-Stats"
url_serieA_shooting_25_26 <- "https://fbref.com/en/comps/11/2025-2026/shooting/2025-2026-Serie-A-Stats"
prem_shooting <- scrape_fbref_shooting(url_prem_shooting_25_26, pause_sec = 3)
scrape_fbref_table <- function(league_url, table_id = "stats_shooting", pause_sec = 3) {
message("Fetching: ", league_url)
# download raw HTML
page <- read_html(league_url)
# FBref often hides tables inside HTML comments
comments <- page %>%
html_nodes(xpath = "//comment()") %>%
html_text()
# Look for the comment chunk that contains our table_id
tbl_html <- comments[str_detect(comments, table_id)]
if (length(tbl_html) == 0) {
stop(paste("Could not find", table_id, "table in commented HTML"))
}
# Some pages may have more than one hit; take the first
tbl_page <- read_html(tbl_html[1])
# Parse the table
tbl <- tbl_page %>%
html_element(paste0("table#", table_id)) %>%
html_table(fill = TRUE)
# Clean up
tbl_clean <- tbl %>%
clean_names() %>%
filter(!player %in% c("Player", NA)) %>%
mutate(across(where(is.character), str_trim))
Sys.sleep(pause_sec)
return(tbl_clean)
}
# Example: Premier League & Serie A shooting 2025-26
url_prem <- "https://fbref.com/en/comps/9/2025-2026/shooting/2025-2026-Premier-League-Stats"
url_serieA <- "https://fbref.com/en/comps/11/2025-2026/shooting/2025-2026-Serie-A-Stats"
prem_shooting <- scrape_fbref_table(url_prem, "stats_shooting")
library(janitor)
# Example: Premier League & Serie A shooting 2025-26
url_prem <- "https://fbref.com/en/comps/9/2025-2026/shooting/2025-2026-Premier-League-Stats"
url_serieA <- "https://fbref.com/en/comps/11/2025-2026/shooting/2025-2026-Serie-A-Stats"
prem_shooting <- scrape_fbref_table(url_prem, "stats_shooting")
library(rvest)
library(dplyr)
library(stringr)
library(janitor)
scrape_fbref_table <- function(league_url, table_id = "stats_shooting", pause_sec = 3) {
message("Fetching: ", league_url)
# download raw HTML
page <- read_html(league_url)
# FBref often hides tables inside HTML comments
comments <- page %>%
html_nodes(xpath = "//comment()") %>%
html_text()
# Look for the comment chunk that contains our table_id
tbl_html <- comments[str_detect(comments, table_id)]
if (length(tbl_html) == 0) {
stop(paste("Could not find", table_id, "table in commented HTML"))
}
# Some pages may have more than one hit; take the first
tbl_page <- read_html(tbl_html[1])
# Parse the table
tbl <- tbl_page %>%
html_element(paste0("table#", table_id)) %>%
html_table(fill = TRUE)
# Clean up
tbl_clean <- tbl %>%
clean_names()
# Drop duplicate header rows if a "player" (or similar) column exists
if ("player" %in% names(tbl_clean)) {
tbl_clean <- tbl_clean %>% filter(!player %in% c("Player", NA))
} else if ("name" %in% names(tbl_clean)) {
tbl_clean <- tbl_clean %>% filter(!name %in% c("Player", NA))
}
tbl_clean <- tbl_clean %>%
mutate(across(where(is.character), str_trim))
Sys.sleep(pause_sec)
return(tbl_clean)
}
# Example: Premier League 2025-26
url_prem <- "https://fbref.com/en/comps/9/2025-2026/shooting/2025-2026-Premier-League-Stats"
prem_shooting <- scrape_fbref_table(url_prem, "stats_shooting")
names(prem_shooting)
head(prem_shooting, 5)
library(rvest)
library(dplyr)
library(stringr)
library(janitor)
scrape_fbref_table <- function(league_url, table_id = "stats_shooting", pause_sec = 3) {
message("Fetching: ", league_url)
page <- read_html(league_url)
comments <- page %>%
html_nodes(xpath = "//comment()") %>%
html_text()
tbl_html <- comments[str_detect(comments, table_id)]
if (length(tbl_html) == 0) {
stop(paste("Could not find", table_id, "table in commented HTML"))
}
tbl_page <- read_html(tbl_html[1])
raw_tbl <- tbl_page %>%
html_element(paste0("table#", table_id)) %>%
html_table(fill = TRUE)
# --- Fix column names ---
# First row is spanners, second row is actual headers
header <- raw_tbl[1, ] |> unlist() |> as.character()
names(raw_tbl) <- header
# Drop the first header row
tbl <- raw_tbl[-1, ]
# Clean and tidy
tbl_clean <- tbl %>%
clean_names() %>%
filter(!is.na(player), player != "Player") %>%
mutate(across(where(is.character), str_trim))
Sys.sleep(pause_sec)
return(tbl_clean)
}
# Example: Premier League 2025-26
url_prem <- "https://fbref.com/en/comps/9/2025-2026/shooting/2025-2026-Premier-League-Stats"
prem_shooting <- scrape_fbref_table(url_prem, "stats_shooting")
names(prem_shooting)
head(prem_shooting, 5)
View(prem_shooting)
library(googlesheets4)
library(tidyverse)
library(janitor)
cat("====================================\n")
cat("XFP DATA UPDATE SCRIPT\n")
cat("Run this weekly to update app data\n")
cat("====================================\n\n")
# Source sheet URL (your main data source)
source_sheet_url <- "https://docs.google.com/spreadsheets/d/1gULbmQ5vmaeoxovra-I1pWP6GlI0tLhqx6NuUCG-uEc/edit?gid=1280900748#gid=1280900748"
# Target PUBLIC sheet URL (where processed data goes)
target_sheet_url <- "https://docs.google.com/spreadsheets/d/1N8Dn8d-3OiqjqATvsz9d79buHBSdz3chjp4wMtd47es/edit?gid=0#gid=0"
# Download source data ====
cat("Step 1: Downloading source data from Google Sheets...\n")
weekly_air_yards <- read_sheet(source_sheet_url, sheet = "WeeklyAirYards") %>%
clean_names()
weekly_rb_xp <- read_sheet(source_sheet_url, sheet = "WeeklyRBXP") %>%
clean_names()
cat("✓ Source data downloaded successfully.\n\n")
# Process combined_xfp_data ====
cat("Step 2: Processing combined xFP data...\n")
# Process WR/TE/QB data
wr_te_qb_data <- weekly_air_yards %>%
filter(position != "RB") %>%
mutate(
player = receiver,
xFP = ifelse(!is.na(knn_fps), knn_fps, 0),
actual_FP = ppr_points,
carries = 0,
rush_yards = 0,
rush_touchdowns = 0,
goal_line_carry = 0
) %>%
select(
player, position, team, week, season, receiver_id,
xFP, actual_FP,
target, air_yards, rz_target, receptions,
receiving_yards, receiving_touchdowns,
carries, rush_yards, rush_touchdowns, goal_line_carry,
adot, catch_rate, ypt
)
# Process RB data
rb_combined_data <- weekly_rb_xp %>%
mutate(
player = player,
position = "RB",
team = team,
xFP = knn_fps,
actual_FP = actual_fps,
target = targets,
rz_target = NA_real_,
receptions = NA_real_,
receiving_yards = NA_real_,
receiving_touchdowns = NA_real_,
receiver_id = paste0(player, "-", team),
adot = NA_real_,
catch_rate = NA_real_,
ypt = NA_real_
) %>%
select(
player, position, team, week, season, receiver_id,
xFP, actual_FP,
target, air_yards, rz_target, receptions,
receiving_yards, receiving_touchdowns,
carries, rush_yards, rush_touchdowns, goal_line_carry,
adot, catch_rate, ypt
)
# Combine and standardize
combined_xfp_data <- bind_rows(wr_te_qb_data, rb_combined_data) %>%
mutate(
# Standardize team names
team = case_when(
team == "LA" ~ "LAR",
team == "STL" ~ "LAR",
team == "SD" ~ "LAC",
team == "OAK" ~ "LV",
TRUE ~ team
),
# Add calculated fields
xFP = as.numeric(xFP),
actual_FP = as.numeric(actual_FP),
fp_diff = actual_FP - xFP,
games = 1
) %>%
arrange(team, week, desc(xFP))
cat("✓ Combined xFP data processed.\n")
cat("  - Total rows:", nrow(combined_xfp_data), "\n")
cat("  - Unique players:", n_distinct(combined_xfp_data$player), "\n")
cat("  - Weeks included:", paste(sort(unique(combined_xfp_data$week)), collapse = ", "), "\n\n")
# Upload to public Google Sheet ====
cat("Step 3: Uploading processed data to public Google Sheet...\n")
# Clear existing sheet and write new data
sheet_write(
combined_xfp_data,
ss = target_sheet_url,
sheet = "combined_xfp_data"
)
cat("✓ Data successfully uploaded to Google Sheet.\n\n")
# Optional: Save local backup
cat("Step 4: Saving local backup...\n")
dir.create("data", showWarnings = FALSE)
write_csv(combined_xfp_data, "data/combined_xfp_data_backup.csv")
cat("✓ Local backup saved to data/combined_xfp_data_backup.csv\n\n")
# Summary
cat("====================================\n")
cat("UPDATE COMPLETE!\n")
cat("====================================\n")
cat("Processed", nrow(combined_xfp_data), "rows of data\n")
cat("Data is now available at:\n")
cat(target_sheet_url, "\n\n")
cat("Your Shiny app will automatically use this updated data.\n")
cat("No need to redeploy the app!\n")
shiny::runApp()
# Source sheet URL (your main data source)
source_sheet_url <- "https://docs.google.com/spreadsheets/d/1gULbmQ5vmaeoxovra-I1pWP6GlI0tLhqx6NuUCG-uEc/edit?gid=1280900748#gid=1280900748"
# Target PUBLIC sheet URL (where processed data goes)
target_sheet_url <- "https://docs.google.com/spreadsheets/d/1N8Dn8d-3OiqjqATvsz9d79buHBSdz3chjp4wMtd47es/edit?gid=0#gid=0"
# Download source data ====
cat("Step 1: Downloading source data from Google Sheets...\n")
weekly_air_yards <- read_sheet(source_sheet_url, sheet = "WeeklyAirYards") %>%
clean_names()
library(googlesheets4)
library(tidyverse)
library(janitor)
library(googlesheets4)
library(tidyverse)
library(janitor)
library(googlesheets4)
library(tidyverse)
library(janitor)
library(janitor)
cat("====================================\n")
cat("XFP DATA UPDATE SCRIPT\n")
cat("Run this weekly to update app data\n")
cat("====================================\n\n")
# Source sheet URL (your main data source)
source_sheet_url <- "https://docs.google.com/spreadsheets/d/1gULbmQ5vmaeoxovra-I1pWP6GlI0tLhqx6NuUCG-uEc/edit?gid=1280900748#gid=1280900748"
# Target PUBLIC sheet URL (where processed data goes)
target_sheet_url <- "https://docs.google.com/spreadsheets/d/1N8Dn8d-3OiqjqATvsz9d79buHBSdz3chjp4wMtd47es/edit?gid=0#gid=0"
# Download source data ====
cat("Step 1: Downloading source data from Google Sheets...\n")
weekly_air_yards <- read_sheet(source_sheet_url, sheet = "WeeklyAirYards") %>%
clean_names()
weekly_rb_xp <- read_sheet(source_sheet_url, sheet = "WeeklyRBXP") %>%
clean_names()
cat("✓ Source data downloaded successfully.\n\n")
# Process combined_xfp_data ====
cat("Step 2: Processing combined xFP data...\n")
# Process WR/TE/QB data
wr_te_qb_data <- weekly_air_yards %>%
filter(position != "RB") %>%
mutate(
player = receiver,
xFP = ifelse(!is.na(knn_fps), knn_fps, 0),
actual_FP = ppr_points,
carries = 0,
rush_yards = 0,
rush_touchdowns = 0,
goal_line_carry = 0
) %>%
select(
player, position, team, week, season, receiver_id,
xFP, actual_FP,
target, air_yards, rz_target, receptions,
receiving_yards, receiving_touchdowns,
carries, rush_yards, rush_touchdowns, goal_line_carry,
adot, catch_rate, ypt
)
# Process RB data
rb_combined_data <- weekly_rb_xp %>%
mutate(
player = player,
position = "RB",
team = team,
xFP = knn_fps,
actual_FP = actual_fps,
target = targets,
rz_target = NA_real_,
receptions = NA_real_,
receiving_yards = NA_real_,
receiving_touchdowns = NA_real_,
receiver_id = paste0(player, "-", team),
adot = NA_real_,
catch_rate = NA_real_,
ypt = NA_real_
) %>%
select(
player, position, team, week, season, receiver_id,
xFP, actual_FP,
target, air_yards, rz_target, receptions,
receiving_yards, receiving_touchdowns,
carries, rush_yards, rush_touchdowns, goal_line_carry,
adot, catch_rate, ypt
)
# Combine and standardize
combined_xfp_data <- bind_rows(wr_te_qb_data, rb_combined_data) %>%
mutate(
# Standardize team names
team = case_when(
team == "LA" ~ "LAR",
team == "STL" ~ "LAR",
team == "SD" ~ "LAC",
team == "OAK" ~ "LV",
TRUE ~ team
),
# Add calculated fields
xFP = as.numeric(xFP),
actual_FP = as.numeric(actual_FP),
fp_diff = actual_FP - xFP,
games = 1
) %>%
arrange(team, week, desc(xFP))
cat("✓ Combined xFP data processed.\n")
cat("  - Total rows:", nrow(combined_xfp_data), "\n")
cat("  - Unique players:", n_distinct(combined_xfp_data$player), "\n")
cat("  - Weeks included:", paste(sort(unique(combined_xfp_data$week)), collapse = ", "), "\n\n")
# Upload to public Google Sheet ====
cat("Step 3: Uploading processed data to public Google Sheet...\n")
# Clear existing sheet and write new data
sheet_write(
combined_xfp_data,
ss = target_sheet_url,
sheet = "combined_xfp_data"
)
cat("✓ Data successfully uploaded to Google Sheet.\n\n")
# Optional: Save local backup
cat("Step 4: Saving local backup...\n")
dir.create("data", showWarnings = FALSE)
write_csv(combined_xfp_data, "data/combined_xfp_data_backup.csv")
cat("✓ Local backup saved to data/combined_xfp_data_backup.csv\n\n")
# Summary
cat("====================================\n")
cat("UPDATE COMPLETE!\n")
cat("====================================\n")
cat("Processed", nrow(combined_xfp_data), "rows of data\n")
cat("Data is now available at:\n")
cat(target_sheet_url, "\n\n")
cat("Your Shiny app will automatically use this updated data.\n")
cat("No need to redeploy the app!\n")
